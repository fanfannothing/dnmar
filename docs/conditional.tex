\documentclass[12pt]{article}

\newcommand{\E}[1]{{\mathbf{E}[#1]}}
\newcommand{\Prob}[1]{{\mathbf{Pr}[#1]}}
\newcommand{\ProbSub}[2]{{\mathbf{Pr}_{#1}[#2]}}
\newcommand{\Var}[1]{{\mathbf{Var}[#1]}}
\newcommand{\Cov}[1]{{\mathbf{Cov}[#1]}}

\usepackage[authoryear,round]{natbib}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{comment}
\usepackage{url}

\usepackage{subfigure}

\begin{document}

\date{}
\title{Modeling Missing Data in Distant Supervision}
\author{}
\maketitle

\bibliographystyle{plainnat}

\section{Introduction}
Recent work has explored distant supervision for relation extraction \citep{mintz09,Hoffmann11} where large, noisy training datasets are heuristically
generated by combining unlabeled text corpora and databases.  Previous work has made a \emph{closed world} assumption that facts
stated in the database are false.  In reality, databases are often incomplete; for example a database containing people's places of birth 
is unlikely to contain all instances mentioned in a large text collection, leading to false negatives in
the training data.

To address the issue of false negatives, we propose to model facts missing from the database as hidden variables.  Because the variables which
are hidden depends heavily on their value (the only observed variables are positive), we can no longer make the
\emph{Missing at Random} (MAR) assumption \citep{Schafer02}, and thus need to explicitly model the process by which data are hidden.

An initial observation model might simply assume that the probability of observing a fact given that it is false is zero, whereas
the probability of observation given that the fact is true is some parameter $\theta$.  Because different relations are likely
to have different degrees of coverage (for example we are unlikely to extract any new instances of the {\sc StateCapital} relation),
a slightly more sophisticated model might take the coverage of the relation into account.  Additionally, the ``popularity'' of 
entities involved could influence the probability a fact is observed (presumably few facts involving Barack Obama are missing from
Wikipedia/Freebase).

\section{Model}
As a controlled experiment it may make sense to start from an existing state of the art model (e.g. \cite{Hoffmann11}), replacing
facts missing in the database with hidden variables, and introducing an observation model.  Below is a brief overview of how this might work in practice.

\subsection{List of variables}
\begin{center}
\begin{tabular}{|c|p{4in}|}
\hline
$(e_1,e_2)$ & Entity pair \\
\hline
$\mathbf{x}^{(e_1,e_2)}_i$ & (Observed) features for mention $i$ of pair $(e_1,e_2)$ \\
\hline
$z^{(e_1,e_2)}_i$ & (Hidden) relation expressed in mention $i$ or {\tt NIL} if no relation is expressed (categorical) \\
\hline
$r^{(e_1,e_2)}_k$ & (Partially Observed) variable indicating whether relation $k$ holds between $e_1$ and $e_2$. \\
\hline
$o^{(e_1,e_2)}_k$ & (Observed) variable indicating whether $r^{(e_1,e_2)}_k$ is observed or hidden. \\
\hline
\end{tabular}
\end{center}

\subsection{Conditional Likelihood}
\label{conditional_likelihood}
\begin{eqnarray*}
  P(\mathbf{z}, \mathbf{r}, \mathbf{o}|\mathbf{x},\theta,\gamma) & = & \prod_{(e_1,e_2)} \left[ \prod_i \left[ P(z^{(e_1,e_2)}_i|\mathbf{x}^{(e_1,e_2)}_i;\theta) \right] \prod_{k} \left[P(r^{(e_1,e_2)}_k|\mathbf{z}) P(o^{(e_1,e_2)}_k|r^{(e_1,e_2)}_k;\gamma) \right] \right]\\
\end{eqnarray*}

\subsection{Local Probability Models}
\subsubsection{Sentence Level Factors}
\[
P(z^{(e_1,e_2)}_i=k|\mathbf{x}^{(e_1,e_2)}_i) \propto \text{exp}\left(\theta \cdot f(k,\mathbf{x}^{(e_1,e_2)}_i)\right)
\]

\subsubsection{Aggregate Factors}
Following \cite{Hoffmann11}, we could define these factors using a deterministic \emph{or}:
\[
P(r^{(e_1,e_2)}_k=1|\mathbf{z}^{(e_1,e_2)}) =
\left\{
\begin{array}{ll}
  1  & \mbox{if } k \in \mathbf{z}^{(e_1,e_2)} \\
  0 & \mbox{otherwise } 
  \end{array}
\right.
\]

\subsubsection{Observation Model}
\[
P(o^{(e_1,e_2)}_k=1|r^{(e_1,e_2)}_k) = 
\left\{
\begin{array}{ll}
  0 & \mbox{if } r^{(e_1,e_2)}_k = 0 \\
  \text{sigmoid}\left(\gamma \cdot f(1,r^{(e_1,e_2)}_k)\right) & \mbox{otherwise }  \\
  \end{array}
\right.
\]

\subsection{Parameter Updates}
To optimize the conditional likelihood (\ref{conditional_likelihood}), we can perform gradient ascent in the parameter space.  Following is the gradient with respect
to the parameter associated with feature $f_i$:
\begin{eqnarray*}
  \frac{\partial}{\partial \theta_i} l(\theta:\mathcal{D}) & = & \mathbf{E}_{P(\mathbf{z},\mathbf{r}_{\text{hidden}}|\mathbf{r}_{\text{observed}}, \mathbf{o}, \mathbf{x})}[f_i] - \mathbf{E}_{P(\mathbf{z},\mathbf{r}_{\text{hidden}}, \mathbf{r}_{\text{observed}}, \mathbf{o} | \mathbf{x})}[f_i] \\
\end{eqnarray*}

Computing these expectations will require running inference in the model, which is most likely too expensive to do exactly.
We can either use a single assignment to hidden variables which (approximately) maximizes likelihood given the current parameters as done by \cite{Hoffmann11}, or possibly
combine this with gibbs sampling to estimate expectations.

\bibliography{bib}
\end{document}

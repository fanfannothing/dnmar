\documentclass[12pt]{article}

\newcommand{\E}[1]{{\mathbf{E}[#1]}}
\newcommand{\Prob}[1]{{\mathbf{Pr}[#1]}}
\newcommand{\ProbSub}[2]{{\mathbf{Pr}_{#1}[#2]}}
\newcommand{\Var}[1]{{\mathbf{Var}[#1]}}
\newcommand{\Cov}[1]{{\mathbf{Cov}[#1]}}

\usepackage[authoryear,round]{natbib}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{comment}
\usepackage{url}

\usepackage{subfigure}

\begin{document}

\date{}
\title{Modeling Missing Data in Distant Supervision}
\author{}
\maketitle

\bibliographystyle{plainnat}

\section{Introduction}
Most previous work in relation extraction assumes fully observed training data (e.g. sentences annotated with the relations they express).
Recently there has been a trend towards leveraging open-domain databases of facts such as Freebase \footnote{http://www.freebase.com/} in combination with un-annotated text corpora to heuristically 
generate large (but noisy) datasets for training relation extractors \citep{mintz09,Hoffmann11}.

Previous work has made a \emph{closed world} assumption that facts not stated in the database are false.  
This assumption is problematic, because databases are often incomplete; for example a database 
containing people's places of birth is unlikely to contain all instances mentioned in a large text collection, leading to false negatives in
the training data.

To address the issue of false negatives, we propose to explicitly model the possibility that a fact missing from the database could
be true.  Because the truth of a fact is highly correlated with it's probability of inclusion in the database, the
\emph{Missing at Random} (MAR) assumption is not valid  \citep{Schafer02}, and we therefore need to explicitly model
the process by which data are included.

An initial observation model might simply assume that the probability of observing a fact (in the database) given that it is false is zero, whereas
the probability of observation given that the fact is true is some parameter $\theta$.  Because different relations are likely
to have different degrees of coverage (for example we are unlikely to extract any new instances of the {\sc StateCapital} relation),
a slightly more sophisticated model might take the coverage of the relation into account.  Additionally, the ``popularity'' of 
entities involved influences the probability a fact is observed; presumably few facts involving Barack Obama are missing from
Wikipedia/Freebase, whereas facts about less prominent entities are more likely to be omitted.

\section{Model}
As a controlled experiment it may make sense to start from an existing state of the art model (e.g. \cite{Hoffmann11}), replacing
facts missing in the database with hidden variables, and introducing an observation model.  Below is a brief overview of how this might work in practice.

\subsection{List of variables}
\begin{center}
\begin{tabular}{|c|p{2in}|p{2in}|}
\hline
Variable & Description & Example Value \\
\hline
\hline
$(e_1,e_2)$ & Entity pair & (Barak Obama, Honolulu)\\
\hline
$\mathbf{x}^{(e_1,e_2)}_i$ & (Observed) sentence for mention $i$ of pair $(e_1,e_2)$ & Barak Obama grew up in Honolulu. \\
\hline
$z^{(e_1,e_2)}_i$ & (Hidden) relation expressed in mention $i$ or {\tt NIL} if no relation is expressed (categorical) & $k$ (indexes a relation, e.g. {\sc LivedIn($x,y$)}) \\
\hline
$r^{(e_1,e_2)}_k$ & (Partially Observed) variable indicating whether relation $k$ holds between $e_1$ and $e_2$ (binary). & {\sc True} \\
\hline
$o^{(e_1,e_2)}_k$ & (Observed) variable indicating whether $r^{(e_1,e_2)}_k$ is observed or hidden (binary). & {\sc True} \\
\hline
\end{tabular}
\end{center}

\subsection{Conditional Likelihood}
\label{conditional_likelihood}
\begin{eqnarray*}
  P(\mathbf{z}, \mathbf{r}, \mathbf{o}|\mathbf{x},\theta,\phi) & = & \prod_{(e_1,e_2)} \left[ \prod_i \left[ P(z^{(e_1,e_2)}_i|\mathbf{x}^{(e_1,e_2)}_i;\theta) \right] \prod_{k} \left[P(r^{(e_1,e_2)}_k|\mathbf{z}) P(o^{(e_1,e_2)}_k|r^{(e_1,e_2)}_k;\phi) \right] \right]\\
\end{eqnarray*}

\subsection{Local Probability Models}
\subsubsection{Sentence Level Factors}
\[
P(z^{(e_1,e_2)}_i=k|\mathbf{x}^{(e_1,e_2)}_i) \propto \text{exp}\left(\theta \cdot f(k,\mathbf{x}^{(e_1,e_2)}_i)\right)
\]

$f(k,\mathbf{x}^{(e_1,e_2)}_i)$ is a vector of binary variables indicating the presence / absence of a wide range of textual patterns which
could be correlated with relation $r_k$.  These include features based on a dependency parser, part of speech tags, etc... and
are highly correlated (e.g. different parts of a dependency path are not independent).  
For example, given the sentence:
\begin{quotation}
  The inn is not far from the boyhood home of the author {\bf Thomas Wolfe}, who was born and raised in {\bf Asheville} and used his mother's boarding house as the setting for the novel ``Look Homeward, Angel''.
\end{quotation}
A few example features are listed below:
\begin{verbatim}
|PERSON|, who was born and raised in|LOCATION|
|PERSON|, who was born and raised in|LOCATION|and 
author[NMOD]->|PERSON|[PMOD]<-was[NMOD]<-born[VC]<-in[ADV]<-|LOCATION|
of[NMOD]<-|PERSON|[PMOD]<-was[NMOD]<-born[VC]<-in[ADV]<-|LOCATION|
\end{verbatim}

\subsubsection{Aggregate Factors}
\label{deterministic_or}
Following \cite{Hoffmann11}, we could define these factors using a deterministic \emph{or}:
\[
P(r^{(e_1,e_2)}_k=1|\mathbf{z}^{(e_1,e_2)}) =
\left\{
\begin{array}{ll}
  1  & \mbox{if } k \in \mathbf{z}^{(e_1,e_2)} \\
  0 & \mbox{otherwise } 
  \end{array}
\right.
\]

\subsubsection{Observation Model}
\[
P(o^{(e_1,e_2)}_k=1|r^{(e_1,e_2)}_k) = 
\left\{
\begin{array}{ll}
  0 & \mbox{if } r^{(e_1,e_2)}_k = 0 \\
  \text{sigmoid}\left(\phi \cdot f(k,e_1,e_2)\right) & \mbox{otherwise }  \\
  \end{array}
\right.
\]

\subsection{Parameter Updates}
To optimize the conditional likelihood (\ref{conditional_likelihood}), we can perform gradient ascent in the parameter space.  Following is the gradient with respect
to the parameter associated with feature $f_i$:
\begin{eqnarray*}
  \frac{\partial}{\partial \theta_i} l(\theta:\mathcal{D}) & = & \mathbf{E}_{P(\mathbf{z},\mathbf{r}_{\text{hidden}}|\mathbf{r}_{\text{observed}}, \mathbf{o}, \mathbf{x})}[f_i] - \mathbf{E}_{P(\mathbf{z},\mathbf{r}_{\text{hidden}}, \mathbf{r}_{\text{observed}}, \mathbf{o} | \mathbf{x})}[f_i] \\
\end{eqnarray*}

Computing these expectations will require running inference in the model, which is most likely too expensive to do exactly.
We can either use a single assignment to hidden variables which (approximately) maximizes likelihood given the current parameters as done by \cite{Hoffmann11}, or possibly
combine this with gibbs sampling to estimate expectations.

\section{Discussion}
Note that according to the model, a fact is true with probability 1 given that it is observed:
\begin{eqnarray*}
P(r^{(e_1,e_2)}_k=0|o^{(e_1,e_2)}_k=1) & \propto & P(o^{(e_1,e_2)}_k=1|r^{(e_1,e_2)}_k=0) \cdot P(r^{(e_1,e_2)}_k=0) \\
& = & 0 \cdot P(r^{(e_1,e_2)}_k=0) \\
& = & 0 \\
\end{eqnarray*}
So in some sense the ``observed'' variables $\mathbf{r}_{\text{observed}}$ can be considered as hidden variables whose value is entirely determined by the (observed) observation variables $\mathbf{o}$.
I think this is probably just a matter of terminology (are variables whose value is fully determined by observed variables observed or hidden?).  In any event the relationship between
$\mathbf{r_\text{observed}}$ and $\mathbf{o}$ is informative for learning parameters of the observation model (e.g. if many facts appear in Freebase about Barak Obama, then we can learn that the probability of a
true fact about Obama being observed in the database is relatively high.

\section{Possible Extensions}
In addition to modeling facts missing from the database, it could be beneficial to explicitly model which facts are missing / not stated in the text collection.  
As described the \emph{deterministic or}
factor \S \ref{deterministic_or}, assumes that each true fact is stated at least once in the text, which is an unrealistic assumption in practice.  Many entity pairs are mentioned
only once, and a single mention is unlikely to express a specific relation.  Modeling missing data in the text would be somewhat different from missing data in the database, since
the observation variable $\mathbf{o_\text{text}}$ would be hidden (unobserved) in this case.

\bibliography{bib}
\end{document}
